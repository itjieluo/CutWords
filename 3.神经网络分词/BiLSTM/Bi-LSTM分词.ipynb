{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read origin data\n",
    "text = open('data/data.txt', encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['人/b  们/e  常/s  说/s  生/b  活/e  是/s  一/s  部/s  教/b  科/m  书/e  ',\n",
       " '  而/s  血/s  与/s  火/s  的/s  战/b  争/e  更/s  是/s  不/b  可/m  多/m  得/e  的/s  教/b  科/m  书/e  ']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 去除标点符号的词位，按照标点符号划分句子\n",
    "sentences = re.split('[，。！？、‘’“”]/[bems]', text)\n",
    "sentences[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去除长度为0的句子\n",
    "sentences = list(filter(lambda x: x.strip(), sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip sentences\n",
    "sentences = list(map(lambda x: x.strip(), sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start creating words and labels...\n",
      "Words Length 321533 Labels Length 321533\n",
      "Words Example ['人' '们' '常' '说' '生' '活' '是' '一' '部' '教' '科' '书']\n",
      "Labels Example ['b' 'e' 's' 's' 'b' 'e' 's' 's' 's' 'b' 'm' 'e']\n"
     ]
    }
   ],
   "source": [
    "# To numpy array\n",
    "words, labels = [], []\n",
    "print('Start creating words and labels...')\n",
    "for sentence in sentences:\n",
    "    groups = re.findall('(.)/(.)', sentence)\n",
    "    arrays = np.asarray(groups)\n",
    "    words.append(arrays[:, 0])\n",
    "    labels.append(arrays[:, 1])\n",
    "print('Words Length', len(words), 'Labels Length', len(labels))\n",
    "print('Words Example', words[0])\n",
    "print('Labels Example', labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all words\n",
    "all_words = list(chain(*words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All words to Series\n",
    "all_words_sr = pd.Series(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get value count, index changed to set\n",
    "all_words_counts = all_words_sr.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get words set\n",
    "all_words_set = all_words_counts.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get words ids\n",
    "all_words_ids = range(1, len(all_words_set) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict to transform\n",
    "word2id = pd.Series(all_words_ids, index=all_words_set)\n",
    "id2word = pd.Series(all_words_set, index=all_words_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag set and ids\n",
    "tags_set = ['x', 's', 'b', 'm', 'e']\n",
    "tags_ids = range(len(tags_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict to transform\n",
    "tag2id = pd.Series(tags_ids, index=tags_set)\n",
    "id2tag = pd.Series(tags_set, index=tag2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 32\n",
    "\n",
    "def x_transform(words):\n",
    "    ids = list(word2id[words])\n",
    "    if len(ids) >= max_length:\n",
    "        ids = ids[:max_length]#截断，把多余的扔掉\n",
    "    ids.extend([0] * (max_length - len(ids)))#padding,不够的在后面补0\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_transform(tags):\n",
    "    ids = list(tag2id[tags])\n",
    "    if len(ids) >= max_length:\n",
    "        ids = ids[:max_length]#截断，把多余的扔掉\n",
    "    ids.extend([0] * (max_length - len(ids)))#padding,不够的在后面补0\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting transform...\n"
     ]
    }
   ],
   "source": [
    "print('Starting transform...')\n",
    "data_x = list(map(lambda x: x_transform(x), words))\n",
    "data_y = list(map(lambda y: y_transform(y), labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data X Length 321533 Data Y Length 321533\n",
      "Data X Example [8, 43, 320, 88, 36, 198, 7, 2, 41, 163, 124, 245, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Data Y Example [2, 4, 1, 1, 2, 4, 1, 1, 1, 2, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print('Data X Length', len(data_x), 'Data Y Length', len(data_y))\n",
    "print('Data X Example', data_x[0])\n",
    "print('Data Y Example', data_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = np.asarray(data_x)\n",
    "data_y = np.asarray(data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pickle to file...\n",
      "Pickle finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from os import makedirs\n",
    "from os.path import exists, join\n",
    "\n",
    "path = 'data/'\n",
    "\n",
    "if not exists(path):\n",
    "    makedirs(path)\n",
    "\n",
    "print('Starting pickle to file...')\n",
    "with open(join(path, 'data.pkl'), 'wb') as f:\n",
    "    pickle.dump(data_x, f)\n",
    "    pickle.dump(data_y, f)\n",
    "    pickle.dump(word2id, f)\n",
    "    pickle.dump(id2word, f)\n",
    "    pickle.dump(tag2id, f)\n",
    "    pickle.dump(id2tag, f)\n",
    "print('Pickle finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from os.path import join\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from keras import backend as K#返回当前后端\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers import Embedding,LSTM,Layer,initializers,regularizers,constraints,Input,Dropout,concatenate,BatchNormalization\n",
    "from keras.layers import Dense,Bidirectional,Concatenate,Multiply,Maximum,Subtract,Lambda,dot,Flatten,Reshape,TimeDistributed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Load data from pickle\n",
    "    :return: Arrays\n",
    "    \"\"\"\n",
    "    with open(path, 'rb') as f:\n",
    "        data_x = pickle.load(f)\n",
    "        data_y = pickle.load(f)\n",
    "        word2id = pickle.load(f)\n",
    "        id2word = pickle.load(f)\n",
    "        tag2id = pickle.load(f)\n",
    "        id2tag = pickle.load(f)\n",
    "        return data_x, data_y, word2id, id2word, tag2id, id2tag\n",
    "\n",
    "\n",
    "def get_data(data_x, data_y):\n",
    "    \"\"\"\n",
    "    Split data from loaded data\n",
    "    :param data_x:\n",
    "    :param data_y:\n",
    "    :return: Arrays\n",
    "    \"\"\"\n",
    "    print('Data X Length', len(data_x), 'Data Y Length', len(data_y))\n",
    "    print('Data X Example', data_x[0])\n",
    "    print('Data Y Example', data_y[0])\n",
    "    \n",
    "    train_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size=0.2, random_state=40)\n",
    "    train_x, dev_x, train_y, dev_y = train_test_split(train_x, train_y, test_size=0.2, random_state=40)\n",
    "    \n",
    "    print('Train X Shape', train_x.shape, 'Train Y Shape', train_y.shape)\n",
    "    print('Dev X Shape', dev_x.shape, 'Dev Y Shape', dev_y.shape)\n",
    "    print('Test X Shape', test_x.shape, 'Test Y Shape', test_y.shape)\n",
    "    return train_x, train_y, dev_x, dev_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bi_LSTM_mdoel(embedding_matrix,embedding_size = 100,max_sentence_length = 32):\n",
    "    #定义模型输入\n",
    "    input_layer = Input(shape=(max_sentence_length,), dtype='int32')\n",
    "    \n",
    "    # 定义需要使用的网络层\n",
    "    embedding_layer = Embedding(\n",
    "        input_dim=len(embedding_matrix, ),\n",
    "        output_dim=embedding_size,\n",
    "        weights=[embedding_matrix],\n",
    "        trainable=True,\n",
    "        input_length=max_sentence_length\n",
    "    )(input_layer)\n",
    "\n",
    "    blstm = Bidirectional(LSTM(64, return_sequences=True), merge_mode='sum')(embedding_layer)\n",
    "    output = TimeDistributed(Dense(5, activation='softmax'))(blstm)\n",
    "    model = Model(input=input_layer, output=output, name=\"bi_lstm_token\")\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#配置参数\n",
    "max_length = 32\n",
    "RANOD_SEED = 42\n",
    "np.random.seed(RANOD_SEED)\n",
    "nepoch = 1\n",
    "batch_size = 1024\n",
    "embedding_size = 100\n",
    "\n",
    "model_checkpoint_path = 'save_model.h5' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train():\n",
    "    # Load data\n",
    "    data_x, data_y, word2id, id2word, tag2id, id2tag = load_data(\"data/data.pkl\")\n",
    "    # Split data\n",
    "    train_x, train_y, dev_x, dev_y, test_x, test_y = get_data(data_x, data_y)\n",
    "    \n",
    "    # 生成适合模型输入的格式\n",
    "    def trans_one(x):\n",
    "        _ = map(lambda y: to_categorical(y, 5), x)\n",
    "        return list(_)\n",
    "    \n",
    "    train_y = np.array(trans_one(train_y))\n",
    "    dev_y = np.array(trans_one(dev_y))\n",
    "    test_y = np.array(trans_one(test_y))\n",
    "    \n",
    "    embedding_matrix = 1 * np.random.randn(len(all_words_set) + 1, embedding_size)\n",
    "    \n",
    "    model = create_bi_LSTM_mdoel(embedding_matrix)\n",
    "\n",
    "    model.fit(x = train_x,y = train_y.reshape((-1, max_length, 5)),\n",
    "                                validation_data = (dev_x,dev_y.reshape((-1, max_length, 5))),\n",
    "                                batch_size = batch_size,\n",
    "                                epochs = nepoch,\n",
    "                                verbose = 1,\n",
    "                                callbacks=[\n",
    "                                EarlyStopping(\n",
    "                                monitor='val_acc',   #监控的方式：’acc’,’val_acc’,’loss’,’val_loss’\n",
    "                                min_delta=0.005,     #增大或者减小的阈值，只有只有大于这个部分才算作improvement\n",
    "                                patience=4,          #连续n次没有提升\n",
    "                                verbose=1,           #信息展示模式\n",
    "                                mode='max'           #‘auto’，‘min’，‘max’之一，在min模式下，如果检测值停止下降则中止训练。在max模式下，当检测值不再上升则停止训练。\n",
    "                                ),\n",
    "                                ModelCheckpoint(\n",
    "                                model_checkpoint_path,\n",
    "                                monitor='val_acc',\n",
    "                                save_best_only=True,\n",
    "                                save_weights_only=False,\n",
    "                                verbose=1,\n",
    "                                mode = \"max\"\n",
    "                                )]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data X Length 321533 Data Y Length 321533\n",
      "Data X Example [  8  43 320  88  36 198   7   2  41 163 124 245   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "Data Y Example [2 4 1 1 2 4 1 1 1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Train X Shape (205780, 32) Train Y Shape (205780, 32)\n",
      "Dev X Shape (51446, 32) Dev Y Shape (51446, 32)\n",
      "Test X Shape (64307, 32) Test Y Shape (64307, 32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86153\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"bi_lstm_token\", inputs=Tensor(\"in..., outputs=Tensor(\"ti...)`\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 32, 100)           515900    \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 32, 64)            84480     \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 32, 5)             325       \n",
      "=================================================================\n",
      "Total params: 600,705\n",
      "Trainable params: 600,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 205780 samples, validate on 51446 samples\n",
      "Epoch 1/1\n",
      "205780/205780 [==============================] - 126s 615us/step - loss: 0.4009 - acc: 0.8366 - val_loss: 0.2647 - val_acc: 0.9012\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.90119, saving model to save_model.h5\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(text):\n",
    "    embedding_matrix = 1 * np.random.randn(len(all_words_set) + 1, embedding_size)\n",
    "    model = create_bi_LSTM_mdoel(embedding_matrix)\n",
    "    model.load_weights(model_checkpoint_path)\n",
    "    tmp = []\n",
    "    tmp.append(list(text))\n",
    "    data_x = list(map(lambda x: x_transform(x),tmp))\n",
    "    data_x = np.array(data_x)\n",
    "    y_test_p = model.predict(data_x,verbose= 0)\n",
    "    print(y_test_p)\n",
    "    tags_set = ['x', 's', 'b', 'm', 'e']\n",
    "    tmp = 0\n",
    "    last_result = []\n",
    "    index = 0\n",
    "    for i in y_test_p:\n",
    "        for j in i:\n",
    "            for k in range(5):\n",
    "                if j[k] > tmp:\n",
    "                    tmp = j[k]\n",
    "                    index = k\n",
    "            last_result.append(tags_set[index])\n",
    "            tmp = 0\n",
    "            index = 0\n",
    "        print(last_result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86153\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"bi_lstm_token\", inputs=Tensor(\"in..., outputs=Tensor(\"ti...)`\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "embedding_8 (Embedding)      (None, 32, 100)           515900    \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 32, 64)            84480     \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 32, 5)             325       \n",
      "=================================================================\n",
      "Total params: 600,705\n",
      "Trainable params: 600,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[[[2.40861485e-03 1.55661225e-01 6.66465342e-01 1.31809250e-01\n",
      "   4.36555780e-02]\n",
      "  [1.38541916e-03 2.71124132e-02 2.06591696e-01 2.97193259e-01\n",
      "   4.67717260e-01]\n",
      "  [1.49074697e-03 9.14590061e-02 3.95683050e-01 4.31627184e-01\n",
      "   7.97400028e-02]\n",
      "  [3.06123588e-03 1.32293357e-02 7.69798458e-02 6.72842503e-01\n",
      "   2.33887106e-01]\n",
      "  [5.65218460e-03 1.73208863e-02 2.02625468e-01 7.12631702e-01\n",
      "   6.17697872e-02]\n",
      "  [2.86056716e-02 9.40187946e-02 8.39752145e-03 5.91793172e-02\n",
      "   8.09798658e-01]\n",
      "  [9.95215178e-01 1.86174712e-03 1.83802171e-04 4.15965565e-04\n",
      "   2.32328661e-03]\n",
      "  [9.99498010e-01 2.22204704e-04 3.03158868e-05 3.33652933e-05\n",
      "   2.16166998e-04]\n",
      "  [9.99819338e-01 7.60218390e-05 1.27408393e-05 1.04642095e-05\n",
      "   8.13682564e-05]\n",
      "  [9.99878883e-01 4.94812557e-05 9.26361190e-06 6.28051339e-06\n",
      "   5.59937325e-05]\n",
      "  [9.99897242e-01 4.16153016e-05 8.23051232e-06 5.01143813e-06\n",
      "   4.78766815e-05]\n",
      "  [9.99904633e-01 3.85921630e-05 7.84198437e-06 4.51301912e-06\n",
      "   4.43654171e-05]\n",
      "  [9.99908328e-01 3.71843271e-05 7.66650646e-06 4.28153680e-06\n",
      "   4.24954087e-05]\n",
      "  [9.99910355e-01 3.64197986e-05 7.57640282e-06 4.15835757e-06\n",
      "   4.13577691e-05]\n",
      "  [9.99911785e-01 3.59573969e-05 7.52558071e-06 4.08534470e-06\n",
      "   4.06160325e-05]\n",
      "  [9.99912739e-01 3.56577111e-05 7.49479022e-06 4.03898548e-06\n",
      "   4.01078287e-05]\n",
      "  [9.99913335e-01 3.54420408e-05 7.47342528e-06 4.00704403e-06\n",
      "   3.97443000e-05]\n",
      "  [9.99913812e-01 3.52645766e-05 7.45502757e-06 3.98292559e-06\n",
      "   3.94725757e-05]\n",
      "  [9.99914408e-01 3.50934279e-05 7.43508735e-06 3.96244241e-06\n",
      "   3.92569636e-05]\n",
      "  [9.99914646e-01 3.48982649e-05 7.40916175e-06 3.94213521e-06\n",
      "   3.90691821e-05]\n",
      "  [9.99915123e-01 3.46428860e-05 7.37143046e-06 3.91826916e-06\n",
      "   3.88832377e-05]\n",
      "  [9.99915838e-01 3.42755411e-05 7.31390355e-06 3.88598937e-06\n",
      "   3.86661668e-05]\n",
      "  [9.99916792e-01 3.37262936e-05 7.22294681e-06 3.83834367e-06\n",
      "   3.83867846e-05]\n",
      "  [9.99918222e-01 3.28866845e-05 7.08376592e-06 3.76531125e-06\n",
      "   3.79737539e-05]\n",
      "  [9.99920368e-01 3.16343321e-05 6.87821603e-06 3.65319829e-06\n",
      "   3.74081101e-05]\n",
      "  [9.99923468e-01 2.98437262e-05 6.58397312e-06 3.48996537e-06\n",
      "   3.66737004e-05]\n",
      "  [9.99927402e-01 2.74437443e-05 6.20328910e-06 3.27137036e-06\n",
      "   3.57354766e-05]\n",
      "  [9.99931931e-01 2.46080326e-05 5.83870269e-06 3.03396519e-06\n",
      "   3.46916386e-05]\n",
      "  [9.99935865e-01 2.17323450e-05 5.77762466e-06 2.89916875e-06\n",
      "   3.38712453e-05]\n",
      "  [9.99934793e-01 2.09342379e-05 7.39898769e-06 3.37838287e-06\n",
      "   3.34817996e-05]\n",
      "  [9.99902248e-01 3.00453048e-05 1.95321227e-05 7.96159293e-06\n",
      "   4.02279584e-05]\n",
      "  [9.99604881e-01 8.90852534e-05 1.38485746e-04 6.49274007e-05\n",
      "   1.02566730e-04]]]\n",
      "['b', 'e', 'm', 'm', 'm', 'e', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x']\n"
     ]
    }
   ],
   "source": [
    "cut(\"中国人民万岁\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
